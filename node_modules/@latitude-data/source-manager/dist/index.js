import { emptyMetadata, compile, readMetadata } from '@latitude-data/sql-compiler';
export { CompileError } from '@latitude-data/sql-compiler';
import 'dotenv/config';
import * as fs from 'fs';
import fs__default from 'fs';
import path from 'path';
import { createHash } from 'crypto';
import QueryResult, { DataType } from '@latitude-data/query_result';
import yaml from 'yaml';
import { ParquetWriter, ParquetSchema } from '@dsnp/parquetjs';

var ConnectorType;
(function (ConnectorType) {
    ConnectorType["Athena"] = "athena";
    ConnectorType["Clickhouse"] = "clickhouse";
    ConnectorType["Duckdb"] = "duckdb";
    ConnectorType["Postgres"] = "postgres";
    ConnectorType["Bigquery"] = "bigquery";
    ConnectorType["Mysql"] = "mysql";
    ConnectorType["Redshift"] = "redshift";
    ConnectorType["Snowflake"] = "snowflake";
    ConnectorType["Trino"] = "trino";
    ConnectorType["Sqlite"] = "sqlite";
    ConnectorType["Mssql"] = "mssql";
    ConnectorType["Databricks"] = "databricks";
    ConnectorType["Test"] = "test";
    ConnectorType["TestInternal"] = "internal_test";
})(ConnectorType || (ConnectorType = {}));
class ConnectorError extends Error {
}
class ConnectionError extends ConnectorError {
}
class QueryError extends ConnectorError {
}
class NotFoundError extends Error {
}
class QueryNotFoundError extends NotFoundError {
}
class SourceFileNotFoundError extends NotFoundError {
}
// Parquet logical types
// https://github.com/LibertyDSNP/parquetjs?tab=readme-ov-file#list-of-supported-types--encodings
var ParquetLogicalType;
(function (ParquetLogicalType) {
    ParquetLogicalType["BOOLEAN"] = "BOOLEAN";
    ParquetLogicalType["INT32"] = "INT32";
    ParquetLogicalType["INT64"] = "INT64";
    ParquetLogicalType["INT96"] = "INT96";
    ParquetLogicalType["FLOAT"] = "FLOAT";
    ParquetLogicalType["DOUBLE"] = "DOUBLE";
    ParquetLogicalType["BYTE_ARRAY"] = "BYTE_ARRAY";
    ParquetLogicalType["FIXED_LEN_BYTE_ARRAY"] = "FIXED_LEN_BYTE_ARRAY";
    ParquetLogicalType["UTF8"] = "UTF8";
    ParquetLogicalType["ENUM"] = "ENUM";
    ParquetLogicalType["DATE"] = "DATE";
    ParquetLogicalType["TIME_MILLIS"] = "TIME_MILLIS";
    ParquetLogicalType["TIMESTAMP_MILLIS"] = "TIMESTAMP_MILLIS";
    ParquetLogicalType["TIMESTAMP_MICROS"] = "TIMESTAMP_MICROS";
    ParquetLogicalType["TIME_MICROS"] = "TIME_MICROS";
    ParquetLogicalType["UINT_8"] = "UINT_8";
    ParquetLogicalType["UINT_16"] = "UINT_16";
    ParquetLogicalType["UINT_32"] = "UINT_32";
    ParquetLogicalType["UINT_64"] = "UINT_64";
    ParquetLogicalType["INT_8"] = "INT_8";
    ParquetLogicalType["INT_16"] = "INT_16";
    ParquetLogicalType["INT_32"] = "INT_32";
    ParquetLogicalType["INT_64"] = "INT_64";
    ParquetLogicalType["JSON"] = "JSON";
    ParquetLogicalType["BSON"] = "BSON";
    ParquetLogicalType["INTERVAL"] = "INTERVAL";
})(ParquetLogicalType || (ParquetLogicalType = {}));

function findSourceConfigFromQuery({ query, queriesDir, }) {
    const fullPath = path.join(queriesDir, query.endsWith('.sql') ? query : `${query}.sql`);
    if (!fs.existsSync(fullPath)) {
        throw new QueryNotFoundError(`Query file not found at ${fullPath}`);
    }
    try {
        fs.accessSync(fullPath);
    }
    catch (e) {
        throw new QueryNotFoundError(`Query file not found at ${fullPath}`);
    }
    // Start from the directory of the .sql file and iterate upwards.
    let currentDir = path.dirname(fullPath);
    while (currentDir.includes(queriesDir)) {
        // Stop if the root directory is reached
        // Try to find a .yml file in the current directory
        const files = fs.readdirSync(currentDir);
        const ymlFile = files?.find((file) => file.endsWith('.yml') || file.endsWith('.yaml'));
        if (ymlFile) {
            // Assume a YML file is the source configuration file
            return path.join(currentDir, ymlFile);
        }
        // Move up one directory
        currentDir = path.dirname(currentDir);
    }
    throw new SourceFileNotFoundError(`Source file not found at ${fullPath}`);
}

const buildCastMethod = (_) => ({
    requirements: {
        interpolationPolicy: 'allow', // Results can be interpolated
        interpolationMethod: 'parameterize', // When interpolating, use parameterization
        requireStaticArguments: false, // Can be used with variables or logic expressions
    },
    resolve: async (value, type) => {
        if (!(type in CAST_METHODS)) {
            throw new Error(`Unsupported cast type: '${type}'. Supported types are: ${Object.keys(CAST_METHODS).join(', ')}`);
        }
        return CAST_METHODS[type](value);
    },
    readMetadata: async () => {
        return emptyMetadata();
    },
});
const CAST_METHODS = {
    string: (value) => String(value),
    text: (value) => String(value),
    int: (value) => parseInt(value),
    float: (value) => parseFloat(value),
    number: (value) => Number(value),
    bool: (value) => Boolean(value),
    boolean: (value) => Boolean(value),
    date: (value) => new Date(value),
};

const buildInterpolateMethod = (_) => ({
    requirements: {
        interpolationPolicy: 'require', // Cannot be used inside a logic block
        interpolationMethod: 'raw', // When interpolating, will just inject the value directly into the query
        requireStaticArguments: false, // Can be used with variables or logic expressions
    },
    resolve: async (value) => {
        return String(value);
    },
    readMetadata: async () => {
        return emptyMetadata();
    },
});

const buildParamMethod = ({ context, }) => ({
    requirements: {
        interpolationPolicy: 'allow', // Results can be interpolated
        interpolationMethod: 'parameterize', // When interpolating, use parameterization
        requireStaticArguments: false, // Can be used with variables or logic expressions
    },
    resolve: async (name, defaultValue) => {
        const requestParams = context.request.params ?? {};
        if (typeof name !== 'string')
            throw new Error('Invalid parameter name');
        if (!(name in requestParams) && defaultValue === undefined) {
            throw new Error(`Missing parameter '${name}' in request`);
        }
        if (name in requestParams) {
            context.accessedParams[name] = requestParams[name];
        }
        return name in requestParams ? requestParams[name] : defaultValue;
    },
    readMetadata: async () => {
        return emptyMetadata();
    },
});

function getFullQueryPath({ referencedQueryPath, currentQueryPath, }) {
    return referencedQueryPath.startsWith('/')
        ? referencedQueryPath
        : path.join(path.dirname(currentQueryPath), referencedQueryPath);
}
function assertNoCyclicReferences(queryPath, queriesBeingCompiled) {
    const queryName = queryPath.replace(/.sql$/, '');
    if (!queriesBeingCompiled.includes(queryName))
        return;
    throw new Error('Query reference to a parent, resulting in cyclic references.');
}

const buildRefMethod = ({ source, context, }) => ({
    requirements: {
        interpolationPolicy: 'require', // Cannot be used inside a logic block
        interpolationMethod: 'raw', // When interpolating, will just inject the returned value directly into the query
        requireStaticArguments: true, // Can only use static arguments
    },
    resolve: async (referencedQuery) => {
        if (typeof referencedQuery !== 'string') {
            throw new Error('Invalid reference query');
        }
        const fullSubQueryPath = getFullQueryPath({
            referencedQueryPath: referencedQuery,
            currentQueryPath: context.request.queryPath,
        });
        assertNoCyclicReferences(fullSubQueryPath, context.queriesBeingCompiled);
        const refSource = await source.manager.loadFromQuery(fullSubQueryPath);
        if (refSource !== source) {
            throw new Error('Query reference to a different source');
        }
        const refRequest = {
            queryPath: fullSubQueryPath,
            params: context.request.params,
        };
        const { request: _, ...refContext } = context; // Everything except request is passed to the subquery context
        const compiledSubQuery = await source.compileQuery(refRequest, refContext);
        return `(${compiledSubQuery.sql})`;
    },
    readMetadata: async (args) => {
        const [referencedQuery] = args;
        const fullSubQueryPath = getFullQueryPath({
            referencedQueryPath: referencedQuery,
            currentQueryPath: context.request.queryPath,
        });
        const { rawSql, ...rest } = await source.getMetadataFromQuery(fullSubQueryPath);
        const sqlHash = createHash('sha256').update(rawSql).digest('hex');
        return {
            ...rest,
            config: emptyMetadata().config, // The config from the referenced query is not relevant for the parent query
            sqlHash,
        };
    },
});

const buildRunQueryMethod = ({ source, context, }) => ({
    requirements: {
        interpolationPolicy: 'disallow', // Cannot be directly interpolated
        interpolationMethod: 'parameterize', // When interpolating, use parameterization
        requireStaticArguments: false, // Can only use static arguments
    },
    resolve: async (referencedQuery) => {
        if (typeof referencedQuery !== 'string') {
            throw new Error('Invalid reference query');
        }
        const fullSubQueryPath = getFullQueryPath({
            referencedQueryPath: referencedQuery,
            currentQueryPath: context.request.queryPath,
        });
        if (fullSubQueryPath in context.ranQueries) {
            return context.ranQueries[fullSubQueryPath];
        }
        assertNoCyclicReferences(fullSubQueryPath, context.queriesBeingCompiled);
        const refSource = await source.manager.loadFromQuery(fullSubQueryPath);
        const refRequest = {
            queryPath: fullSubQueryPath,
            params: context.request.params,
        };
        const refContext = {
            accessedParams: context.accessedParams,
            resolvedParams: [], // Subquery must not access nor modify the parent query's resolved params
            ranQueries: context.ranQueries,
            queriesBeingCompiled: context.queriesBeingCompiled,
        };
        const compiledSubQuery = await refSource.compileQuery(refRequest, refContext);
        const results = await refSource
            .runCompiledQuery(compiledSubQuery)
            .then((res) => res.toArray());
        context.ranQueries[fullSubQueryPath] = results;
        return results;
    },
    readMetadata: async () => {
        return emptyMetadata();
    },
});

function buildSupportedMethods(args) {
    return {
        interpolate: buildInterpolateMethod(),
        param: buildParamMethod(args),
        ref: buildRefMethod(args),
        runQuery: buildRunQueryMethod(args),
        cast: buildCastMethod(),
    };
}

class BaseConnector {
    source;
    constructor({ source }) {
        this.source = source;
    }
    /**
     * The connector is able to perform streaming (batching) of the results
     * of a query. If the connector does not define this method it will throw
     */
    async batchQuery(_c, _o) {
        throw new Error('Batching not supported');
    }
    /**
     * Close the connection to the data source.
     * This method must only be called once all queries have been executed and the
     * connector instance is no longer needed.
     */
    async end() { }
    /**
     * Compile the given query. This method returns the compiled SQL, resolved parameters,
     * and information about the compilation process.
     */
    async compileQuery(context) {
        const { request, resolvedParams, accessedParams, queriesBeingCompiled } = context;
        queriesBeingCompiled.push(request.queryPath);
        const resolveFn = async (value) => {
            const resolved = this.resolve(value, resolvedParams.length);
            resolvedParams.push(resolved);
            return resolved.resolvedAs;
        };
        const supportedMethods = this.buildSupportedMethods({
            source: this.source,
            context,
        });
        const compiledSql = await compile({
            supportedMethods,
            query: request.sql,
            resolveFn,
        });
        // NOTE: To avoid compiling subqueries that have already
        // been compiled in the current call stack.
        queriesBeingCompiled.pop();
        return {
            sql: compiledSql,
            resolvedParams,
            accessedParams,
        };
    }
    /**
     * Parses the query and returns the configuration defined and methods used.
     * This definition is static, and only depends on the contents of the query
     * itself.
     */
    readMetadata(request) {
        // The supported methods object is only needed for their keys, but the actual
        // function implementations are not used, since they won't be called in this
        // process. Since some functions require context-specific information just to
        // be defined, they are mocked in this case.
        const supportedMethods = this.buildSupportedMethods({
            context: {
                request,
                accessedParams: {},
                resolvedParams: [],
                ranQueries: {},
                queriesBeingCompiled: [],
            },
            source: this.source,
        });
        return readMetadata({
            query: request.sql,
            supportedMethods,
        });
    }
    runCompiled(request) {
        return this.runQuery(request);
    }
    buildSupportedMethods(buildArgs) {
        return buildSupportedMethods(buildArgs);
    }
}

/**
 * This is a mock connector that can be used for testing purposes.
 * It does not connect to any database and returns a result based on
 * the input query.
 *
 * Each line in the query will represent a new row in the result, as
 * a 'value' field.
 * Resolved values are shown in the result as `[[value]]`
 *
 * A line can also start with one of the following keywords to define
 * different behaviors:
 * - `FAIL <message>` will throw an error with the given message.
 * - `SLEEP <ms>` will wait for the given number of milliseconds.
 */
class TestConnector extends BaseConnector {
    onResolve;
    onRunQuery;
    constructor(options) {
        super(options);
        const connectionParams = options.connectionParams;
        if (connectionParams?.fail) {
            throw new ConnectionError(connectionParams.fail);
        }
        this.onResolve = connectionParams?.onResolve;
        this.onRunQuery = connectionParams?.onRunQuery;
    }
    resolve(value) {
        const resolvedParam = {
            value,
            resolvedAs: `[[${value}]]`,
        };
        this?.onResolve?.(value, resolvedParam);
        return resolvedParam;
    }
    async batchQuery(compiledQuery, options) {
        return Promise.reject(new Error(`
        batchQuery not implemented for TestConnector
        Mock it in your tests
        CompiledQuery: ${JSON.stringify(compiledQuery)}
        batchOptions: ${JSON.stringify(options)}
      `));
    }
    async runQuery(compiledQuery) {
        const lines = compiledQuery.sql.split('\n');
        const rows = [];
        const commands = ['FAIL', 'SLEEP'];
        for (const line of lines) {
            const command = commands.find((c) => line.trim().startsWith(c));
            const content = line
                .trim()
                .slice(command?.length)
                .trim();
            if (command === 'FAIL') {
                this?.onRunQuery?.(compiledQuery, undefined, new ConnectorError(content));
                throw new ConnectorError(content);
            }
            if (command === 'SLEEP') {
                const time = parseInt(content, 10);
                await new Promise((resolve) => setTimeout(resolve, time));
                continue;
            }
            rows.push(content);
        }
        const result = new QueryResult({
            fields: [{ name: 'value', type: DataType.String }],
            rows: rows.map((value) => [value]),
            rowCount: rows.length,
        });
        if (this.onRunQuery)
            this.onRunQuery(compiledQuery, result);
        return result;
    }
}

const CONNECTOR_PACKAGES = {
    [ConnectorType.Postgres]: 'postgresql-connector',
    [ConnectorType.Redshift]: 'postgresql-connector',
    [ConnectorType.Clickhouse]: 'clickhouse-connector',
    [ConnectorType.Bigquery]: 'bigquery-connector',
    [ConnectorType.Mysql]: 'mysql-connector',
    [ConnectorType.Snowflake]: 'snowflake-connector',
    [ConnectorType.Athena]: 'athena-connector',
    [ConnectorType.Trino]: 'trino-connector',
    [ConnectorType.Duckdb]: 'duckdb-connector',
    [ConnectorType.Sqlite]: 'sqlite-connector',
    [ConnectorType.Mssql]: 'mssql-connector',
    [ConnectorType.Databricks]: 'databricks-connector',
    [ConnectorType.Test]: 'test-connector',
};
class ConnectorNotInstalledError extends Error {
    constructor(pkgName) {
        const message = `Module ${pkgName} is not a valid Latitude connector. Please make sure you have the correct package installed in your project by running 'npm install ${pkgName}'`;
        super(message);
    }
}
class ConnectorWithoutDefaultExportError extends Error {
    constructor(pkgName) {
        const message = `Module ${pkgName} does not have a default export.`;
        super(message);
    }
}
async function importConnector(packageName) {
    try {
        const module = await import(/* @vite-ignore */ packageName);
        const ConnectorClass = module.default;
        if (typeof ConnectorClass === 'function' &&
            ConnectorClass.prototype.constructor) {
            return ConnectorClass;
        }
        else {
            throw new ConnectorWithoutDefaultExportError(packageName);
        }
    }
    catch (error) {
        if (error instanceof ConnectorWithoutDefaultExportError) {
            throw error;
        }
        throw new ConnectorNotInstalledError(packageName);
    }
}
class InvalidConnectorType extends Error {
}
function getConnectorPackage(type) {
    if (type === ConnectorType.TestInternal)
        return null;
    if (!(type in CONNECTOR_PACKAGES)) {
        throw new InvalidConnectorType(`Unsupported connector type: ${type}`);
    }
    const pkgName = CONNECTOR_PACKAGES[type];
    return `@latitude-data/${pkgName}`;
}
async function createConnectorFactory({ type, connectorOptions, }) {
    const packageName = getConnectorPackage(type);
    const ConnectorClass = !packageName
        ? TestConnector // If no package is found, use the test connector
        : await importConnector(packageName);
    return new ConnectorClass(connectorOptions);
}

function buildDefaultContext() {
    return {
        accessedParams: {},
        resolvedParams: [],
        ranQueries: {},
        queriesBeingCompiled: [],
    };
}
class Source {
    _schema;
    _connector;
    path;
    manager;
    constructor({ path, schema, sourceManager, connector, }) {
        this.path = path;
        this._schema = schema;
        this.manager = sourceManager;
        this._connector = connector;
    }
    get config() {
        return this._schema?.config ?? {};
    }
    get type() {
        return this._schema.type;
    }
    get connectionParams() {
        return this._schema.details ?? {};
    }
    get connectorPackageName() {
        const pkgName = getConnectorPackage(this.type);
        return pkgName ?? ConnectorType.TestInternal;
    }
    async endConnection() {
        if (!this._connector)
            return;
        await this._connector.end();
        this._connector = undefined;
    }
    async getMetadataFromQuery(queryPath) {
        const sql = await this.getSql(queryPath);
        const connector = await this.connector();
        const request = { queryPath, sql, params: {} };
        const { config: queryConfig, ...rest } = await connector.readMetadata(request);
        return {
            config: {
                ...this.config,
                ...queryConfig,
            },
            ...rest,
        };
    }
    async compileQuery({ queryPath, params }, context) {
        const sql = await this.getSql(queryPath);
        const connector = await this.connector();
        const defaultContext = buildDefaultContext();
        const fullContext = {
            ...defaultContext,
            ...context,
            request: { queryPath, sql, params: params ?? {} },
        };
        return await connector.compileQuery(fullContext);
    }
    async runCompiledQuery(compiledQuery) {
        const connector = await this.connector();
        return await connector.runCompiled(compiledQuery);
    }
    async batchQuery(compiledQuery, options) {
        const connector = await this.connector();
        return connector.batchQuery(compiledQuery, options);
    }
    async connector() {
        if (!this._connector) {
            this._connector = await createConnectorFactory({
                type: this.type,
                connectorOptions: {
                    source: this,
                    connectionParams: this.connectionParams,
                },
            });
        }
        return this._connector;
    }
    async getSql(queryPath) {
        const querySource = await this.manager.loadFromQuery(queryPath);
        if (this !== querySource) {
            throw new ConnectionError(`Query path "${queryPath}" is not in source "${this.path}"`);
        }
        const cleanPath = queryPath.replace(/^\//, '');
        const sqlPath = path.resolve(this.manager.queriesDir, cleanPath.endsWith('.sql') ? cleanPath : `${cleanPath}.sql`);
        return fs.readFileSync(sqlPath, 'utf8');
    }
}

class InvalidSourceConfigError extends Error {
    constructor(message) {
        super(message);
        this.name = 'InvalidSourceConfigError';
    }
}
function readSourceConfig(sourcePath) {
    if (!fs__default.existsSync(sourcePath)) {
        throw new SourceFileNotFoundError(`Source file not found at ${sourcePath}`);
    }
    const file = fs__default.readFileSync(sourcePath, 'utf8');
    const config = yaml.parse(file, (_, value) => {
        // if key starts with 'LATITUDE__', replace it with the environment variable
        if (typeof value === 'string' && value.startsWith('LATITUDE__')) {
            if (process.env[value])
                return process.env[value];
            throw new Error(`
      Invalid configuration. Environment variable ${value} was not found in the environment. You can review how to set up secret source credentials in the documentation: https://docs.latitude.so/sources/credentials
      `);
        }
        else {
            return value;
        }
    });
    // Validation requirements
    if (!config?.type) {
        throw new InvalidSourceConfigError(`Missing 'type' in configuration`);
    }
    if (typeof config.type !== 'string') {
        throw new InvalidSourceConfigError(`Invalid 'type' in configuration`);
    }
    return config;
}

class MaterializedFileNotFoundError extends Error {
}
function mapDataTypeToParquet(dataType) {
    switch (dataType) {
        case DataType.Boolean:
            return ParquetLogicalType.BOOLEAN;
        case DataType.Datetime:
            return ParquetLogicalType.TIMESTAMP_MICROS;
        case DataType.Integer:
            // TODO: review this decision. I faced an issue with an int8 column
            // in postgresql which is a BigInt. This library does not support it.
            // it breaks saying:
            //   Cannot convert a BigInt value to a number ðŸ’¥
            // If we put here INT64 it works but not sure is the best approach.
            return ParquetLogicalType.INT32;
        case DataType.Float:
            return ParquetLogicalType.FLOAT;
        case DataType.String:
            return ParquetLogicalType.UTF8;
        default:
            return ParquetLogicalType.BYTE_ARRAY;
    }
}
const ROW_GROUP_SIZE = 4096; // PARQUET BATCH WRITE
/**
 * In order to hash a SQL query, we need to know the source path
 * it came from. This way we ensure the path is unique even
 * if two sources share the same query.
 */
class StorageDriver {
    manager;
    constructor({ manager }) {
        this.manager = manager;
    }
    async writeParquet({ queryPath, params, batchSize, onDebug, }) {
        let writer;
        const source = await this.manager.loadFromQuery(queryPath);
        const { config, sqlHash } = await source.getMetadataFromQuery(queryPath);
        if (!config.materialize) {
            throw new Error('Query is not configured as materialized');
        }
        const compiled = await source.compileQuery({ queryPath, params });
        let currentHeap = 0;
        return new Promise((resolve) => {
            let filePath;
            let queryRows = 0;
            const size = batchSize ?? 1000;
            source.batchQuery(compiled, {
                batchSize: size,
                onBatch: async (batch) => {
                    if (!writer) {
                        const schema = this.buildParquetSchema(batch.fields);
                        filePath = await this.getUrl({
                            sqlHash: sqlHash,
                            queryName: queryPath,
                            sourcePath: source.path,
                            ignoreMissingFile: true,
                        });
                        writer = await ParquetWriter.openFile(schema, filePath, {
                            rowGroupSize: size > ROW_GROUP_SIZE ? size : ROW_GROUP_SIZE,
                        });
                    }
                    for (const row of batch.rows) {
                        if (onDebug) {
                            let heapUsed = process.memoryUsage().heapUsed;
                            if (heapUsed < currentHeap) {
                                onDebug({
                                    memoryUsageInMb: `${(currentHeap / 1024 / 1024).toFixed(2)} MB`,
                                });
                            }
                            currentHeap = heapUsed;
                        }
                        try {
                            await writer.appendRow(row);
                        }
                        catch {
                            // If for some reason a row writing fails we don't want
                            // to break the process.
                        }
                    }
                    queryRows += batch.rows.length;
                    if (batch.lastBatch) {
                        await writer.close();
                        resolve({ filePath, queryRows });
                    }
                },
            });
        });
    }
    getUrl(args) {
        const name = StorageDriver.hashName(args);
        const filename = `${name}.parquet`;
        return this.resolveUrl({ ...args, filename });
    }
    static hashName({ sqlHash, sourcePath }) {
        const hash = createHash('sha256');
        hash.update(sqlHash);
        hash.update(sourcePath);
        return hash.digest('hex');
    }
    buildParquetSchema(fields) {
        const columns = fields.reduce((schema, field) => {
            const type = mapDataTypeToParquet(field.type);
            schema[field.name] = {
                type,
                optional: true,
                compression: 'SNAPPY',
            };
            return schema;
        }, {});
        return new ParquetSchema(columns);
    }
}

class DummyDriver extends StorageDriver {
    constructor({ manager }) {
        super({ manager });
    }
    get basePath() {
        return '/dummy-base-path';
    }
    getUrl(args) {
        return this.resolveUrl({
            ...args,
            filename: `ENCRYPTED[${args.sqlHash}].parquet`,
        });
    }
    resolveUrl({ filename }) {
        return Promise.resolve(filename);
    }
}

class SourceManager {
    instances = {};
    materializeStorage;
    queriesDir;
    constructor(queriesDir, options = {}) {
        this.queriesDir = queriesDir;
        const materializeKlass = options.materialize?.Klass ?? DummyDriver;
        const commonConfig = { manager: this };
        const config = options.materialize?.config;
        this.materializeStorage = config
            ? new materializeKlass({ ...config, ...commonConfig })
            : new DummyDriver(commonConfig);
    }
    /**
     * Finds the source configuration file in the given path and loads it
     * @param path - The path to any file in the source directory. This could be the source configuration file itself or any other query in the directory.
     */
    async loadFromQuery(query) {
        const filePath = path.join(this.queriesDir, query.endsWith('.sql') ? query : `${query}.sql`);
        if (!filePath.includes(this.queriesDir)) {
            throw new SourceFileNotFoundError(`Query file is not in the queries directory: ${filePath}`);
        }
        if (!fs.existsSync(filePath)) {
            throw new QueryNotFoundError(`Query file not found at ${filePath}`);
        }
        const sourceFilePath = findSourceConfigFromQuery({
            query,
            queriesDir: this.queriesDir,
        });
        const sourcePath = path.relative(this.queriesDir, path.dirname(sourceFilePath));
        if (!this.instances[sourcePath]) {
            this.buildSource({ sourcePath, sourceFile: sourceFilePath });
        }
        return this.instances[sourcePath];
    }
    /**
     * Loads a source from a source configuration file
     * @param sourceFile - The path to the source configuration file
     */
    async loadFromConfigFile(sourceFile) {
        if (!path.isAbsolute(sourceFile)) {
            sourceFile = path.join(this.queriesDir, sourceFile);
        }
        if (!fs.existsSync(sourceFile)) {
            throw new SourceFileNotFoundError(`Source file not found at ${sourceFile}`);
        }
        // If the given path is not in queriesDir, throw an error
        if (!sourceFile.includes(this.queriesDir)) {
            throw new SourceFileNotFoundError(`Source file is not in the queries directory: ${sourceFile}`);
        }
        const sourcePath = path.dirname(sourceFile);
        if (!this.instances[sourcePath]) {
            this.buildSource({ sourcePath, sourceFile });
        }
        return this.instances[sourcePath];
    }
    /**
     * Safely closes the connection to a source and removes the instance from the cache
     */
    async clear(source) {
        await source.endConnection();
        delete this.instances[source.path];
    }
    /**
     * Clears all sources and closes all connections
     */
    async clearAll() {
        await Promise.all(Object.values(this.instances).map((source) => this.clear(source)));
    }
    buildSource({ sourceFile, sourcePath, }) {
        const schema = readSourceConfig(sourceFile);
        this.instances[sourcePath] = new Source({
            path: sourcePath,
            schema,
            sourceManager: this,
        });
    }
}

class DiskDriver extends StorageDriver {
    materializeDir;
    constructor({ path, manager }) {
        super({ manager });
        this.materializeDir = path;
    }
    get basePath() {
        return this.materializeDir;
    }
    resolveUrl({ queryName, filename, ignoreMissingFile = false, }) {
        const filepath = path.join(this.materializeDir, filename);
        if (ignoreMissingFile)
            return Promise.resolve(filepath);
        if (fs__default.existsSync(filepath))
            return Promise.resolve(filepath);
        return Promise.reject(new MaterializedFileNotFoundError(`materialize query not found for: '${queryName}'`));
    }
}

class NoDiskDriverError extends Error {
    constructor() {
        super('Disk driver is required for materializing queries');
    }
}
class NonMaterializableQueryError extends Error {
    constructor(query) {
        super(`Query ${query} is not materializable`);
    }
}
function ensureMaterializeDirExists(storage) {
    if (!(storage instanceof DiskDriver)) {
        throw new NoDiskDriverError();
    }
    const basePath = storage.basePath;
    if (!fs__default.existsSync(basePath)) {
        fs__default.mkdirSync(basePath, { recursive: true });
    }
}
function parseTime({ start, end }) {
    const minuteTime = Math.floor((end - start) / 60000);
    const min = minuteTime.toString().padStart(2, '0');
    const secondsTime = ((end - start) % 60000) / 1000;
    const seconds = minuteTime
        ? secondsTime.toFixed(0).padStart(2, '0')
        : secondsTime.toFixed(2).padStart(5, '0');
    return minuteTime ? `${min}:${seconds} minutes` : `${seconds} seconds`;
}
function humanizeFileSize(bytes) {
    const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0)
        return '0 Byte';
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return parseFloat((bytes / Math.pow(1024, i)).toFixed(2)) + ' ' + sizes[i];
}
function buildInfo({ startTime, endTime, query, filePath, queryRows, }) {
    const file = path.basename(filePath);
    return {
        query,
        queryRows: `${queryRows.toLocaleString('de-DE', {
            style: 'decimal',
            useGrouping: true,
        })} rows`,
        time: parseTime({ start: startTime, end: endTime }),
        fileSize: humanizeFileSize(fs__default.statSync(filePath).size),
        file,
    };
}
function recursiveFindQueriesInDir(rootDir, nextDir) {
    const dir = nextDir ?? rootDir;
    const files = fs__default.readdirSync(dir);
    return files.flatMap((file) => {
        const fullPath = path.join(dir, file);
        const isDir = fs__default.statSync(fullPath).isDirectory();
        if (isDir)
            return recursiveFindQueriesInDir(rootDir, fullPath);
        const relativePath = fullPath.replace(rootDir, '').replace(/^\//, '');
        return fullPath.endsWith('.sql') ? [relativePath] : [];
    });
}
async function isMaterializableQuery({ sourceManager, query, }) {
    const source = await sourceManager.loadFromQuery(query);
    const { config } = await source.getMetadataFromQuery(query);
    return config.materialize === true;
}
async function findQueries({ sourceManager, selected, }) {
    const storage = sourceManager.materializeStorage;
    ensureMaterializeDirExists(storage);
    const queriesDir = sourceManager.queriesDir;
    const allQueries = recursiveFindQueriesInDir(queriesDir);
    // We don' filter queries if user pass a specific list
    // If one of then is not materializable we fail
    if (selected.length > 0) {
        return Promise.all(allQueries
            .filter((query) => selected.some((selectedQuery) => query === selectedQuery || query === `${selectedQuery}.sql`))
            .map(async (query) => {
            const canMaterialize = await isMaterializableQuery({
                sourceManager,
                query,
            });
            if (!canMaterialize) {
                throw new NonMaterializableQueryError(query);
            }
            return query;
        }));
    }
    const queries = await Promise.all(allQueries.map(async (query) => {
        const canMaterialize = await isMaterializableQuery({
            sourceManager,
            query,
        });
        return { query, canMaterialize };
    }));
    return queries.filter((q) => q.canMaterialize).map((q) => q.query);
}
const BATCH_SIZE = 4096;
async function findAndMaterializeQueries({ sourceManager, onStartQuery, onDebug, selectedQueries = [], }) {
    const startTotalTime = performance.now();
    const info = [];
    const result = {
        batchSize: BATCH_SIZE,
        successful: false,
        totalTime: '',
        queriesInfo: [],
    };
    const storage = sourceManager.materializeStorage;
    try {
        const queries = await findQueries({
            sourceManager,
            selected: selectedQueries,
        });
        for (const [index, query] of queries.entries()) {
            const startTime = performance.now();
            onStartQuery?.({ count: queries.length, index: index + 1, query });
            const materialize = await storage.writeParquet({
                onDebug,
                queryPath: query,
                params: {},
                batchSize: BATCH_SIZE,
            });
            const endTime = performance.now();
            info.push(buildInfo({
                startTime,
                endTime,
                query,
                filePath: materialize.filePath,
                queryRows: materialize.queryRows,
            }));
        }
        result.successful = true;
    }
    catch (error) {
        if (process.env.NODE_ENV !== 'test') {
            console.error(error);
        }
        result.successful = false;
    }
    finally {
        const endTotalTime = performance.now();
        const totalTime = parseTime({ start: startTotalTime, end: endTotalTime });
        result.totalTime = totalTime;
        result.queriesInfo = info;
    }
    return result;
}

const STORAGE_TYPES = { disk: 'disk' };
function getDriverKlass({ type, }) {
    switch (type) {
        case STORAGE_TYPES.disk:
            return DiskDriver;
        default: {
            return null;
        }
    }
}

export { BaseConnector, ConnectionError, ConnectorError, ConnectorType, DiskDriver, DummyDriver, NotFoundError, ParquetLogicalType, QueryError, QueryNotFoundError, STORAGE_TYPES, Source, SourceFileNotFoundError, SourceManager, StorageDriver, TestConnector as TestConnectorInternal, buildDefaultContext, findAndMaterializeQueries, getDriverKlass };
//# sourceMappingURL=index.js.map
